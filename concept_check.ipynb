{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_k400_train = '/data/jong980812/project/Video-CBM/data/video_annotation/kinetics400/train.csv'\n",
    "import pandas as pd\n",
    "cleaned = pd.read_csv(label_k400_train, header=None, delimiter=',')\n",
    "label_array = list(cleaned.values[:, 1])\n",
    "label = torch.tensor(label_array,dtype=torch.int32)\n",
    "internvid_image_feat = '/data/jong980812/project/Video-CBM/results/k400/class_names/feature_directly/activation/kinetics400_train_clip_ViT-B16.pt'\n",
    "internvid_text_feat = '/data/jong980812/project/Video-CBM/results/k400/class_names/feature_directly/activation/kinetics400_classes_ViT-B16.pt'\n",
    "internvid_image_feat = torch.load(internvid_image_feat)\n",
    "internvid_text_feat = torch.load(internvid_text_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_k400_val = '/data/jong980812/project/Video-CBM/data/video_annotation/kinetics400/val.csv'\n",
    "import pandas as pd\n",
    "cleaned = pd.read_csv(label_k400_val, header=None, delimiter=',')\n",
    "label_array = list(cleaned.values[:, 1])\n",
    "label = torch.tensor(label_array,dtype=torch.int32)\n",
    "internvid_image_feat = '/data/jong980812/project/Video-CBM/results/k400/class_names/feature_directly/activation/kinetics400_val_clip_ViT-B16.pt'\n",
    "internvid_text_feat = '/data/jong980812/project/Video-CBM/results/k400/class_names/feature_directly/activation/kinetics400_classes_ViT-B16.pt'\n",
    "internvid_image_feat = torch.load(internvid_image_feat)\n",
    "internvid_text_feat = torch.load(internvid_text_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ucf101_train = '/data/jong980812/project/Video-CBM/data/video_annotation/UCF101/train.csv'\n",
    "import pandas as pd\n",
    "cleaned = pd.read_csv(label_ucf101_train, header=None, delimiter=' ')\n",
    "ucf101_train_label_array = list(cleaned.values[:, 1])\n",
    "internvid_image_feat = '/data/jong980812/project/Video-CBM/results/ucf101/class_list/activation/UCF101_train_clip_ViT-B16.pt'\n",
    "internvid_text_feat = '/data/jong980812/project/Video-CBM/results/ucf101/class_list/activation/ucf101_classes_ViT-B16.pt'\n",
    "internvid_image_feat = torch.load(internvid_image_feat)\n",
    "internvid_text_feat = torch.load(internvid_text_feat)\n",
    "label ='/data/jong980812/project/Video-CBM/results/ucf101/class_list/activation/label_train.txt'\n",
    "with open(label,'r',encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "lines = [int(line.strip()) for line in lines]\n",
    "label = torch.tensor(lines)\n",
    "\n",
    "\n",
    "\n",
    "# #!!!\n",
    "# label_ucf101_val = '/data/jong980812/project/Video-CBM/data/video_annotation/UCF101/val.csv'\n",
    "# import pandas as pd\n",
    "# cleaned = pd.read_csv(label_ucf101_val, header=None, delimiter=' ')\n",
    "# ucf101_val_label_array = list(cleaned.values[:, 1])\n",
    "# internvid_image_feat = '/data/jong980812/project/Video-CBM/results/ucf101/class_list/activation/UCF101_val_clip_ViT-B16.pt'\n",
    "# internvid_text_feat = '/data/jong980812/project/Video-CBM/results/ucf101/class_list/activation/ucf101_classes_ViT-B16.pt'\n",
    "# internvid_image_feat = torch.load(internvid_image_feat)\n",
    "# internvid_text_feat = torch.load(internvid_text_feat)\n",
    "# label ='/data/jong980812/project/Video-CBM/results/ucf101/class_list/activation/label_val.txt'\n",
    "# with open(label,'r',encoding='utf-8') as file:\n",
    "#     lines = file.readlines()\n",
    "# lines = [int(line.strip()) for line in lines]\n",
    "# label = torch.tensor(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "internvid_image_feat = '/data/jong980812/project/Video-CBM/results/ucf101/class_list/activation/UCF101_val_clip_ViT-B16.pt'\n",
    "internvid_text_feat = '/data/jong980812/project/Video-CBM/results/ucf101/class_list/activation/ucf101_classes_ViT-B16.pt'\n",
    "label ='/data/jong980812/project/Video-CBM/results/ucf101/class_list/activation/label_train.txt'\n",
    "internvid_image_feat = torch.load(internvid_image_feat)\n",
    "internvid_text_feat = torch.load(internvid_text_feat)\n",
    "# label= torch.load(label)\n",
    "with open(label,'r',encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "lines = [int(line.strip()) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "label = torch.tensor(label_k400_train,dtype=torch.int32)\n",
    "print(label[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([240436, 512]), torch.Size([400, 512]), torch.Size([240436]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internvid_image_feat.shape,internvid_text_feat.shape,label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = torch.matmul(vec1, vec2.T)  # 벡터의 내적\n",
    "    vec1_norm = vec1.norm(dim=1, keepdim=True)  # 이미지 벡터의 크기 계산\n",
    "    vec2_norm = vec2.norm(dim=1, keepdim=True)  # 텍스트 벡터의 크기 계산\n",
    "    return dot_product / (vec1_norm * vec2_norm.T)\n",
    "\n",
    "# 각 이미지에 대해 가장 유사한 텍스트의 인덱스를 찾는 함수\n",
    "def find_most_similar_text(image_feats, text_feats):\n",
    "    similarities = cosine_similarity(image_feats, text_feats)  # 모든 유사도 계산\n",
    "    most_similar_indices = torch.argmax(similarities, dim=1)    # 가장 큰 유사도의 텍스트 인덱스 찾기\n",
    "    return most_similar_indices\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 가장 유사한 텍스트 인덱스 저장\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top3_similar_text(image_feats, text_feats):\n",
    "    similarities = cosine_similarity(image_feats, text_feats)  # 모든 유사도 계산\n",
    "    top3_similarities, top3_indices = torch.topk(similarities, k=10, dim=1)  # 상위 3개의 유사도와 인덱스 찾기\n",
    "    return top3_indices\n",
    "\n",
    "# accuracy를 계산하는 함수\n",
    "def compute_accuracy_top3(image_labels, top3_indices):\n",
    "    correct = 0\n",
    "    for i in range(image_labels.size(0)):\n",
    "        if image_labels[i] in top3_indices[i]:  # 라벨이 top3 인덱스에 있는지 확인\n",
    "            correct += 1\n",
    "    accuracy = correct / image_labels.size(0)\n",
    "    print(f'Number of correct elements: {correct}')\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_text_indices = find_most_similar_text(internvid_image_feat.to(device), internvid_text_feat.to(device))\n",
    "top3_indices = find_top3_similar_text(internvid_image_feat.to(device), internvid_text_feat.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct elements: 9618\n",
      "Accuracy: 48.59%\n"
     ]
    }
   ],
   "source": [
    "most_similar_text_indices=most_similar_text_indices.cpu()\n",
    "correct = (most_similar_text_indices == label)\n",
    "\n",
    "# 맞는 원소의 개수를 구함\n",
    "num_correct = correct.sum().item()\n",
    "\n",
    "# accuracy 계산 (전체 개수 중에서 몇 퍼센트가 맞았는지)\n",
    "accuracy = num_correct / most_similar_text_indices.size(0)\n",
    "\n",
    "print(f'Number of correct elements: {num_correct}')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct elements: 16131\n",
      "Accuracy: 81.49%\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy_top3(label, top3_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6723075368761366\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_text_indices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-cbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
