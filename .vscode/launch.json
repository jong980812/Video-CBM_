{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "train_vmae_kinetics400",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "torch.distributed.launch",
            "args": [
                "--nproc_per_node", "1",
                "--master_port", "3333",
                "/data/psh68380/repos/Video-CBM_two-stream/train_video_cbm_two_stream.py",
                "--data_set","kinetics400", // Video
                "--nb_classes","400",
                "--s_concept_set","data/concept_sets/llava_ver2/k400_object_filtered.txt",
                "--t_concept_set","/data/psh68380/repos/Video-CBM_two-stream/data/concept_sets/human_actions_ver2.txt",
                "--p_concept_set","processed_locations.txt",
                // "--t_concept_set","/data/jong980812/project/Video-CBM/data/concept_sets/ssv2_ost_temporal_concepts.txt",
                // "--t_text_save_name","/data/jong980812/project/Video-CBM_backup/results/ssv2/finetune_internvid_ost_temporal/activation/ssv2_ost_temporal_concepts_ViT-B16.pt",
                "--batch_size","32",
                "--video_anno_path","data/video_annotation/kinetics400",
                // "--data_path","/local_datasets/something-something/something-something-v2-mp4",
                "--print",
                "--activation_dir","debug",
                "--save_dir","debug",
                "--proj_steps","2000",
                "--n_iters","2000",
                "--backbone","vmae_vit_base_patch16_224",
                "--finetune","/data/datasets/video_checkpoint/kinetics400/k400_finetune.pth",
                "--dual_encoder","internvid_200m",
                "--lavila_ckpt","/data/datasets/video_checkpoint/lavila_TSF_B.pth",
                "--interpretability_cutoff","0.5",
                "--clip_cutoff","0.2",
                "--train_mode","triple",
                "--saga_batch_size", "256",
                "--saved_features",
                // "--multiview",
                "--debug","/data/psh68380/repos/Video-CBM_two-stream/result/vmae/kinetics400/triple_final_ver/model/kinetics400_cbm_2024_11_09_21_06",
                // "--intervene",
                // "--monitor_class","338",
                // "--save_contibution",
                // "--mimetics"
            ]
        },
        {
            "name": "train_AIM_kinetics400",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "torch.distributed.launch",
            "args": [
                "--nproc_per_node", "1",
                "--master_port", "3333",
                "/data/psh68380/repos/Video-CBM_two-stream/train_video_cbm_two_stream.py",
                "--data_set","kinetics400", // Video
                "--nb_classes","400",
                "--s_concept_set","data/concept_sets/llava_ver2/k400_object_filtered.txt",
                "--t_concept_set","/data/psh68380/repos/Video-CBM_two-stream/data/concept_sets/human_actions_ver2.txt",
                "--p_concept_set","processed_locations.txt",
                // "--t_concept_set","/data/jong980812/project/Video-CBM/data/concept_sets/ssv2_ost_temporal_concepts.txt",
                // "--t_text_save_name","/data/jong980812/project/Video-CBM_backup/results/ssv2/finetune_internvid_ost_temporal/activation/ssv2_ost_temporal_concepts_ViT-B16.pt",
                "--batch_size","32",
                "--video_anno_path","data/video_annotation/kinetics400",
                // "--data_path","/local_datasets/something-something/something-something-v2-mp4",
                "--print",
                "--activation_dir","debug",
                "--save_dir","debug",
                "--proj_steps","2000",
                "--n_iters","2000",
                "--backbone","AIM",
                "--finetune","/data/datasets/video_checkpoint/kinetics400/AIM_finetune.pth",
                "--dual_encoder","internvid_200m",
                "--lavila_ckpt","/data/datasets/video_checkpoint/lavila_TSF_B.pth",
                "--interpretability_cutoff","0.5",
                "--clip_cutoff","0.2",
                "--train_mode","triple",
                "--saga_batch_size", "256",
                "--saved_features",
                // "--multiview",
                "--debug","/data/psh68380/repos/Video-CBM_two-stream/result/AIM/ucf101/triple_final_ver/model/UCF101_cbm_2024_11_08_14_59",
                // "--intervene",
                // "--monitor_class","396",
                "--save_contibution",
                // "--mimetics"
            ]
        },
        {
            "name": "ucf101",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "torch.distributed.launch",
            "args": [
                "--nproc_per_node", "1",
                "--master_port", "3333",
                "/data/psh68380/repos/Video-CBM_two-stream/train_video_cbm_two_stream.py",
                "--data_set","UCF101", // Video
                "--nb_classes","101",
                "--s_concept_set","data/concept_sets/llava_ver2/k400_object_filtered.txt",
                "--t_concept_set","/data/psh68380/repos/Video-CBM_two-stream/data/concept_sets/human_actions_ver2.txt",
                "--p_concept_set","processed_locations.txt",
                // "--t_concept_set","/data/jong980812/project/Video-CBM/data/concept_sets/ssv2_ost_temporal_concepts.txt",
                // "--t_text_save_name","/data/jong980812/project/Video-CBM_backup/results/ssv2/finetune_internvid_ost_temporal/activation/ssv2_ost_temporal_concepts_ViT-B16.pt",
                "--batch_size","32",
                "--video_anno_path","data/video_annotation/UCF101",
                "--data_path","/local_datasets/ucf101/videos",
                // "--data_path","/local_datasets/something-something/something-something-v2-mp4",
                "--print",
                "--activation_dir","debug",
                "--save_dir","debug",
                "--proj_steps","2000",
                "--n_iters","2000",
                "--interpretability_cutoff","0.5",
                "--clip_cutoff","0.2",
                "--backbone","vmae_vit_base_patch16_224",
                "--finetune","/data/datasets/video_checkpoint/ucf101/ucf_finetune.pth",
                "--dual_encoder","internvid_200m",
                "--lavila_ckpt","/data/datasets/video_checkpoint/lavila_TSF_B.pth",
                "--train_mode","triple",
                // "--multiview",
                "--saved_features",
                "--debug","/data/psh68380/repos/Video-CBM_two-stream/result/vmae/ucf101/triple_final_ver/model/UCF101_cbm_2024_11_08_01_46",
                "--save_contibution"
                
            ]
        },
        {
            "name": "toy",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "torch.distributed.launch",
            "args": [
                "--nproc_per_node", "1",
                "--master_port", "3333",
                "/data/psh68380/repos/Video-CBM_two-stream/train_video_cbm_two_stream.py",
                "--data_set","TOY", // Video
                "--nb_classes","4",
                "--s_concept_set","/data/psh68380/repos/Video-CBM_two-stream/data/concept_sets/toy_object.txt",
                "--t_concept_set","/data/psh68380/repos/Video-CBM_two-stream/data/concept_sets/toy_temporal.txt",
                "--p_concept_set","/data/psh68380/repos/Video-CBM_two-stream/data/concept_sets/toy_place.txt",
                "--batch_size","32",
                "--num_frames", "16",
                "--sampling_rate", "4",
                "--video_anno_path","data/video_annotation/TOY",
                "--data_path","/local_datasets/something-something/something-something-v2-mp4",
                "--print",
                "--activation_dir","debug/toy_real/activation",
                "--save_dir","debug/toy_real/model",
                "--proj_steps","2000",
                "--n_iters","2000",
                "--interpretability_cutoff","0.1",
                "--clip_cutoff","0.1",
                "--backbone","vmae_vit_base_patch16_224",
                "--finetune","/data/psh68380/repos/Video-CBM_two-stream/vmae_ssv2_toy_finetune.pt",
                "--dual_encoder","internvid_200m",
                "--lavila_ckpt","/data/datasets/video_checkpoint/lavila_TSF_B.pth",
                "--train_mode","triple",
                // "--multiview",
                // "--saved_features",                
            ]
        },
        {
            "name": "one-stream",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "module": "torch.distributed.launch",
            "args": [
                "--nproc_per_node", "1",
                "--master_port", "3333",
                "/data/psh68380/repos/Video-CBM_two-stream/train_video_cbm.py",
                "--data_set","kth", // Video
                "--nb_classes","5",
                "--data_path", "/local_datasets/kth/split",
                "--concept_set","/data/psh68380/repos/Video-CBM_two-stream/data/concept_sets/kth_concepts.txt",
                "--batch_size","32",
                "--num_frames", "16",
                "--sampling_rate", "4",
                "--video_anno_path","data/video_annotation/kth",
                "--print",
                "--activation_dir","result/toy/kth/using_mlp/activation",
                "--save_dir","result/toy/kth/using_mlp/model",
                "--proj_steps","2000",
                "--n_iters","2000",
                "--backbone","vmae_vit_base_patch16_224",
                "--finetune","/data/jong980812/project/VideoMAE/result/vmae_kth/ssv2/OUT/checkpoint-best/mp_rank_00_model_states.pt",
                "--lavila_ckpt","/data/datasets/video_checkpoint/lavila_TSF_B.pth",
                "--dual_encoder","internvid_200m",
                "--clip_cutoff","0.",
                "--interpretability_cutoff","0.",
                "--train_mode","single",
                "--saga_batch_size", "256",
                "--hard_label", "data/video_annotation/kth",
                "--use_mlp",
                // "--multiview",
                // "--saved_features"
            ]
        }
    ]
}