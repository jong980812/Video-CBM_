{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import json\n",
    "import cbm\n",
    "import data_utils\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Settings for creating CBM')\n",
    "    # parser.add_argument('--batch_size', default=64, type=int)\n",
    "    # parser.add_argument('--epochs', default=30, type=int)\n",
    "    parser.add_argument('--update_freq', default=1, type=int)\n",
    "    parser.add_argument('--save_ckpt_freq', default=100, type=int)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model', default='vit_base_patch16_224', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--tubelet_size', type=int, default= 2)\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='videos input size')\n",
    "\n",
    "    parser.add_argument('--fc_drop_rate', type=float, default=0.0, metavar='PCT',\n",
    "                        help='Dropout rate (default: 0.)')\n",
    "    parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
    "                        help='Dropout rate (default: 0.)')\n",
    "    parser.add_argument('--attn_drop_rate', type=float, default=0.0, metavar='PCT',\n",
    "                        help='Attention dropout rate (default: 0.)')\n",
    "    parser.add_argument('--drop_path', type=float, default=0.1, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.1)')\n",
    "\n",
    "    parser.add_argument('--disable_eval_during_finetuning', action='store_true', default=False)\n",
    "    parser.add_argument('--model_ema', action='store_true', default=False)\n",
    "    parser.add_argument('--model_ema_decay', type=float, default=0.9999, help='')\n",
    "    parser.add_argument('--model_ema_force_cpu', action='store_true', default=False, help='')\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                        help='Optimizer (default: \"adamw\"')\n",
    "    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                        help='Optimizer Epsilon (default: 1e-8)')\n",
    "    parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                        help='Optimizer Betas (default: None, use opt default)')\n",
    "    parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "    parser.add_argument('--weight_decay_end', type=float, default=None, help=\"\"\"Final value of the\n",
    "        weight decay. We use a cosine schedule for WD and using a larger decay by\n",
    "        the end of training improves performance for ViTs.\"\"\")\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate (default: 1e-3)')\n",
    "    parser.add_argument('--layer_decay', type=float, default=0.75)\n",
    "\n",
    "    parser.add_argument('--warmup_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='warmup learning rate (default: 1e-6)')\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n",
    "\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=5, metavar='N',\n",
    "                        help='epochs to warmup LR, if scheduler supports')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n",
    "                        help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--color_jitter', type=float, default=0.4, metavar='PCT',\n",
    "                        help='Color jitter factor (default: 0.4)')\n",
    "    parser.add_argument('--num_sample', type=int, default=1,\n",
    "                        help='Repeated_aug (default: 2)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m7-n4-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m7-n4-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                        help='Label smoothing (default: 0.1)')\n",
    "    parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "    # Evaluation parameters\n",
    "    parser.add_argument('--crop_pct', type=float, default=None)\n",
    "    parser.add_argument('--short_side_size', type=int, default=224)\n",
    "    parser.add_argument('--test_num_segment', type=int, default=5)\n",
    "    parser.add_argument('--test_num_crop', type=int, default=3)\n",
    "\n",
    "    # Random Erase params\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "    # Mixup params\n",
    "    parser.add_argument('--mixup', type=float, default=0.0,\n",
    "                        help='mixup alpha, mixup enabled if > 0.')\n",
    "    parser.add_argument('--cutmix', type=float, default=0.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0.')\n",
    "    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup_prob', type=float, default=0.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup_switch_prob', type=float, default=0.0,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "    parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # Finetuning params\n",
    "    parser.add_argument('--finetune', default='', help='finetune from checkpoint')\n",
    "    parser.add_argument('--model_key', default='model|module', type=str)\n",
    "    parser.add_argument('--model_prefix', default='', type=str)\n",
    "    parser.add_argument('--init_scale', default=0.001, type=float)\n",
    "    parser.add_argument('--use_checkpoint', action='store_true')\n",
    "    parser.set_defaults(use_checkpoint=False)\n",
    "    parser.add_argument('--use_mean_pooling', action='store_true')\n",
    "    parser.set_defaults(use_mean_pooling=True)\n",
    "    parser.add_argument('--use_cls', action='store_false', dest='use_mean_pooling')\n",
    "\n",
    "    # Dataset parameters\n",
    "\n",
    "    parser.add_argument('--eval_data_path', default=None, type=str,\n",
    "                        help='dataset path for evaluation')\n",
    "    parser.add_argument('--nb_classes', default=400, type=int,\n",
    "                        help='number of the classification types')\n",
    "    parser.add_argument('--imagenet_default_mean_and_std', default=True, action='store_true')\n",
    "    parser.add_argument('--num_segments', type=int, default= 1)\n",
    "    parser.add_argument('--num_frames', type=int, default= 16)\n",
    "    parser.add_argument('--sampling_rate', type=int, default= 4)\n",
    "    parser.add_argument('--data_set', default='Kinetics-400', choices=['kinetics100','kinetics400', 'mini-SSV2','SSV2', 'UCF101', 'HMDB51','image_folder'],\n",
    "                        type=str, help='dataset')\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--log_dir', default=None,\n",
    "                        help='path where to tensorboard log')\n",
    "    # parser.add_argument('--device', default='cuda',\n",
    "    #                     help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='',\n",
    "                        help='resume from checkpoint')\n",
    "    parser.add_argument('--auto_resume', action='store_true')\n",
    "    parser.add_argument('--no_auto_resume', action='store_false', dest='auto_resume')\n",
    "    parser.set_defaults(auto_resume=True)\n",
    "\n",
    "    parser.add_argument('--save_ckpt', action='store_true')\n",
    "    parser.add_argument('--no_save_ckpt', action='store_false', dest='save_ckpt')\n",
    "    parser.set_defaults(save_ckpt=True)\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true',\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--dist_eval', action='store_true', default=False,\n",
    "                        help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "    parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local-rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "\n",
    "    parser.add_argument('--enable_deepspeed', action='store_true', default=False)\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"cifar10\")\n",
    "    parser.add_argument(\"--s_concept_set\", type=str, default=None, \n",
    "                        help=\"path to concept set name\")\n",
    "    parser.add_argument(\"--t_concept_set\", type=str, default=None, \n",
    "                        help=\"path to concept set name\")\n",
    "    parser.add_argument(\"--backbone\", type=str, default=\"clip_RN50\", help=\"Which pretrained model to use as backbone\")\n",
    "    parser.add_argument(\"--clip_name\", type=str, default=\"ViT-B/16\", help=\"Which CLIP model to use\")\n",
    "\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\", help=\"Which device to use\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=512, help=\"Batch size used when saving model/CLIP activations\")\n",
    "    parser.add_argument(\"--saga_batch_size\", type=int, default=256, help=\"Batch size used when fitting final layer\")\n",
    "    parser.add_argument(\"--proj_batch_size\", type=int, default=50000, help=\"Batch size to use when learning projection layer\")\n",
    "\n",
    "    parser.add_argument(\"--feature_layer\", type=str, default='layer4', \n",
    "                        help=\"Which layer to collect activations from. Should be the name of second to last layer in the model\")\n",
    "    parser.add_argument(\"--activation_dir\", type=str, default='saved_activations', help=\"save location for backbone and CLIP activations\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default='saved_models', help=\"where to save trained models\")\n",
    "    parser.add_argument(\"--clip_cutoff\", type=float, default=0.25, help=\"concepts with smaller top5 clip activation will be deleted\")\n",
    "    parser.add_argument(\"--proj_steps\", type=int, default=1000, help=\"how many steps to train the projection layer for\")\n",
    "    parser.add_argument(\"--interpretability_cutoff\", type=float, default=0.45, help=\"concepts with smaller similarity to target concept will be deleted\")\n",
    "    parser.add_argument(\"--lam\", type=float, default=0.0007, help=\"Sparsity regularization parameter, higher->more sparse\")\n",
    "    parser.add_argument(\"--n_iters\", type=int, default=1000, help=\"How many iterations to run the final layer solver for\")\n",
    "    parser.add_argument(\"--print\", action='store_true', help=\"Print all concepts being deleted in this stage\")\n",
    "    parser.add_argument('--data_path', default='data/video_annotation/ucf101', type=str,\n",
    "                        help='dataset path')\n",
    "    parser.add_argument('--video_anno_path',type=str)\n",
    "    parser.add_argument('--center_frame',action='store_true')\n",
    "    parser.add_argument('--no_aug',type=bool,default=False)\n",
    "    parser.add_argument('--saved_features',action='store_true')\n",
    "    parser.add_argument('--dual_encoder', default='clip', choices=['clip', 'lavila', 'internvid'],\n",
    "                        type=str, help='dataset')\n",
    "    parser.add_argument('--dual_encoder_frames',type=int,default=16)\n",
    "    parser.add_argument('--lavila_ckpt',type=str,default=None)\n",
    "    parser.add_argument('--sp_clip',action='store_true')\n",
    "    parser.add_argument('--debug',default=None, help=\"model path\")\n",
    "    parser.add_argument('--intervene',action='store_true')\n",
    "    parser.add_argument('--monitor_class', nargs='+', type=int, default=None, help='classes to monitor for intervention testing')\n",
    "    parser.add_argument('--save_contibution', action='store_true', help='if it is true, save contributions')\n",
    "    parser.add_argument('--mimetics', action='store_true', help='if it is true, save contributions')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_selected_args_from_file(args_file, selected_keys):\n",
    "    # 파일을 읽어서 JSON 파싱\n",
    "    with open(args_file, 'r') as f:\n",
    "        args_dict = json.load(f)  # JSON 형식의 파일을 파이썬 딕셔너리로 변환\n",
    "\n",
    "    # 선택한 key에 해당하는 값만 추출하여 argparse 형식으로 변환\n",
    "    args_list = []\n",
    "    for key in selected_keys:\n",
    "        if key in args_dict:\n",
    "            args_list.append(f'--{key}')  # key를 --key 형식으로 변환\n",
    "            args_list.append(str(args_dict[key]))  # 값을 문자열로 변환해서 리스트에 추가\n",
    "\n",
    "    return args_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_contribution(model, dataset, device, batch_size=250, num_workers=10, args=None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    class_concept =[]\n",
    "    for images, labels in tqdm(DataLoader(dataset, batch_size, num_workers=10, pin_memory=True)):\n",
    "        with torch.no_grad():\n",
    "            outs, concept_activation = model(images.to(device))\n",
    "            pred = torch.argmax(outs, dim=1)\n",
    "            correct += torch.sum(pred.cpu()==labels)\n",
    "            total += len(labels)\n",
    "            pred_weights = model.final.weight[pred]  # shape: (batch, num_concept)\n",
    "            concept_contribution = concept_activation * pred_weights  \n",
    "            for j in range(len(images.shape[0])):\n",
    "                current_label = labels[j]\n",
    "                class_concept[current_label] = concept_contribution[j]\n",
    "    return class_concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_classes(X, max_k=10):\n",
    "    best_k = 2\n",
    "    best_score = -1  # 실루엣 score는 -1에서 1 사이\n",
    "\n",
    "    for k in range(2, max_k + 1):  # 최소 2개 클러스터부터 최대 max_k까지 시도\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=0)\n",
    "        cluster = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, cluster)\n",
    "        print(f\"Silhouette Score for {k} clusters: {score}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_k = k\n",
    "            best_score = score\n",
    "\n",
    "    kmeans = KMeans(n_clusters=best_k, random_state=0)\n",
    "    cluster = kmeans.fit_predict(X)\n",
    "    \n",
    "    print(f\"Best number of clusters: {best_k} with Silhouette Score: {best_score}\")\n",
    "    return cluster, best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clusters_to_file(sample_id, num_cluster, output_file='clusters.txt'):\n",
    "    clusters = {i: [] for i in range(num_cluster)}\n",
    "\n",
    "    # for class_name, label in zip(data.keys(), sample_id):\n",
    "    #     clusters[label].append(class_name)\n",
    "    \n",
    "    with open(output_file, 'w') as file:\n",
    "        for cluster_id, class_names in clusters.items():\n",
    "            file.write(f\"Cluster {cluster_id}:\\n\")\n",
    "            for class_name in class_names:\n",
    "                file.write(f\"  - {class_name}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch size = (16, 16)\n",
      "Load ckpt from /data/datasets/video_checkpoint/kinetics400/k400_finetune.pth\n",
      "Load state_dict by model_key = module\n",
      "\n",
      "\n",
      "\n",
      "*********** VMAE Load ***************\n",
      "done\n",
      "Patch size = (16, 16)\n",
      "Load ckpt from /data/datasets/video_checkpoint/kinetics400/k400_finetune.pth\n",
      "Load state_dict by model_key = module\n",
      "\n",
      "\n",
      "\n",
      "*********** VMAE Load ***************\n",
      "Number of the class = 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/155 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "load_dir = '/data/psh68380/repos/Video-CBM_two-stream/result/vmae/kinetics400/triple_final_ver/model/kinetics400_cbm_2024_11_09_21_06'\n",
    "top_n=20\n",
    "file_path = os.path.join(load_dir, 'contribution.txt')\n",
    "\n",
    "# data = parse_contributions(file_path)\n",
    "# X = prepare_data_for_clustering(data, top_n=top_n)\n",
    "selected_keys = [\n",
    "        'data_set', 'nb_classes', 'video_anno_path', 'data_path', 'lavila_ckpt', 'finetune', 'backbone'\n",
    "    ]\n",
    "args_list = get_selected_args_from_file(os.path.join(load_dir,'args.txt'),selected_keys)  # 파일에서 인자 리스트 생성\n",
    "parser = parse_args()\n",
    "argument = parser.parse_args(args_list)  # 파일에서 읽은 인자로 파싱\n",
    "device = \"cuda\"\n",
    "with open(os.path.join(load_dir, \"args.txt\"), \"r\") as f:\n",
    "    args = json.load(f)\n",
    "dataset = args[\"data_set\"]\n",
    "_, target_preprocess = data_utils.get_target_model(args[\"backbone\"], device,argument)\n",
    "print('done')\n",
    "model,_ = cbm.load_cbm_triple(load_dir, device, argument)\n",
    "\n",
    "val_d_probe = dataset+\"_test\"\n",
    "# cls_file = data_utils.LABEL_FILES[dataset]\n",
    "val_data_t = data_utils.get_data(val_d_probe,args=argument)\n",
    "val_data_t.end_point = 2\n",
    "#############\n",
    "X = get_concept_contribution(model, val_data_t, device)\n",
    "########################\n",
    "clusters, optimal_cluster = cluster_classes(X, max_k=100)\n",
    "cluster_output = os.path.join(load_dir, f'clusters_{optimal_cluster}.txt')\n",
    "save_clusters_to_file(clusters, num_clusters=optimal_cluster, output_file=cluster_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vcbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
