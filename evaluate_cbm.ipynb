{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ca1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Settings for creating CBM')\n",
    "# parser.add_argument('--batch_size', default=64, type=int)\n",
    "# parser.add_argument('--epochs', default=30, type=int)\n",
    "parser.add_argument('--update_freq', default=1, type=int)\n",
    "parser.add_argument('--save_ckpt_freq', default=100, type=int)\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument('--model', default='vit_base_patch16_224', type=str, metavar='MODEL',\n",
    "                    help='Name of model to train')\n",
    "parser.add_argument('--tubelet_size', type=int, default= 2)\n",
    "parser.add_argument('--input_size', default=224, type=int,\n",
    "                    help='videos input size')\n",
    "\n",
    "parser.add_argument('--fc_drop_rate', type=float, default=0.0, metavar='PCT',\n",
    "                    help='Dropout rate (default: 0.)')\n",
    "parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
    "                    help='Dropout rate (default: 0.)')\n",
    "parser.add_argument('--attn_drop_rate', type=float, default=0.0, metavar='PCT',\n",
    "                    help='Attention dropout rate (default: 0.)')\n",
    "parser.add_argument('--drop_path', type=float, default=0.1, metavar='PCT',\n",
    "                    help='Drop path rate (default: 0.1)')\n",
    "\n",
    "parser.add_argument('--disable_eval_during_finetuning', action='store_true', default=False)\n",
    "parser.add_argument('--model_ema', action='store_true', default=False)\n",
    "parser.add_argument('--model_ema_decay', type=float, default=0.9999, help='')\n",
    "parser.add_argument('--model_ema_force_cpu', action='store_true', default=False, help='')\n",
    "\n",
    "# Optimizer parameters\n",
    "parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                    help='Optimizer (default: \"adamw\"')\n",
    "parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                    help='Optimizer Epsilon (default: 1e-8)')\n",
    "parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                    help='Optimizer Betas (default: None, use opt default)')\n",
    "parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n",
    "                    help='Clip gradient norm (default: None, no clipping)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                    help='weight decay (default: 0.05)')\n",
    "parser.add_argument('--weight_decay_end', type=float, default=None, help=\"\"\"Final value of the\n",
    "    weight decay. We use a cosine schedule for WD and using a larger decay by\n",
    "    the end of training improves performance for ViTs.\"\"\")\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                    help='learning rate (default: 1e-3)')\n",
    "parser.add_argument('--layer_decay', type=float, default=0.75)\n",
    "\n",
    "parser.add_argument('--warmup_lr', type=float, default=1e-6, metavar='LR',\n",
    "                    help='warmup learning rate (default: 1e-6)')\n",
    "parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                    help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n",
    "\n",
    "parser.add_argument('--warmup_epochs', type=int, default=5, metavar='N',\n",
    "                    help='epochs to warmup LR, if scheduler supports')\n",
    "parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n",
    "                    help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n",
    "\n",
    "# Augmentation parameters\n",
    "parser.add_argument('--color_jitter', type=float, default=0.4, metavar='PCT',\n",
    "                    help='Color jitter factor (default: 0.4)')\n",
    "parser.add_argument('--num_sample', type=int, default=1,\n",
    "                    help='Repeated_aug (default: 2)')\n",
    "parser.add_argument('--aa', type=str, default='rand-m7-n4-mstd0.5-inc1', metavar='NAME',\n",
    "                    help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m7-n4-mstd0.5-inc1)'),\n",
    "parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                    help='Label smoothing (default: 0.1)')\n",
    "parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                    help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "# Evaluation parameters\n",
    "parser.add_argument('--crop_pct', type=float, default=None)\n",
    "parser.add_argument('--short_side_size', type=int, default=224)\n",
    "parser.add_argument('--test_num_segment', type=int, default=5)\n",
    "parser.add_argument('--test_num_crop', type=int, default=3)\n",
    "\n",
    "# Random Erase params\n",
    "parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                    help='Random erase prob (default: 0.25)')\n",
    "parser.add_argument('--remode', type=str, default='pixel',\n",
    "                    help='Random erase mode (default: \"pixel\")')\n",
    "parser.add_argument('--recount', type=int, default=1,\n",
    "                    help='Random erase count (default: 1)')\n",
    "parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                    help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "# Mixup params\n",
    "parser.add_argument('--mixup', type=float, default=0.0,\n",
    "                    help='mixup alpha, mixup enabled if > 0.')\n",
    "parser.add_argument('--cutmix', type=float, default=0.0,\n",
    "                    help='cutmix alpha, cutmix enabled if > 0.')\n",
    "parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n",
    "                    help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "parser.add_argument('--mixup_prob', type=float, default=0.0,\n",
    "                    help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "parser.add_argument('--mixup_switch_prob', type=float, default=0.0,\n",
    "                    help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "parser.add_argument('--mixup_mode', type=str, default='batch',\n",
    "                    help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "# Finetuning params\n",
    "parser.add_argument('--finetune', default='', help='finetune from checkpoint')\n",
    "parser.add_argument('--model_key', default='model|module', type=str)\n",
    "parser.add_argument('--model_prefix', default='', type=str)\n",
    "parser.add_argument('--init_scale', default=0.001, type=float)\n",
    "parser.add_argument('--use_checkpoint', action='store_true')\n",
    "parser.set_defaults(use_checkpoint=False)\n",
    "parser.add_argument('--use_mean_pooling', action='store_true')\n",
    "parser.set_defaults(use_mean_pooling=True)\n",
    "parser.add_argument('--use_cls', action='store_false', dest='use_mean_pooling')\n",
    "\n",
    "# Dataset parameters\n",
    "\n",
    "parser.add_argument('--eval_data_path', default=None, type=str,\n",
    "                    help='dataset path for evaluation')\n",
    "parser.add_argument('--nb_classes', default=400, type=int,\n",
    "                    help='number of the classification types')\n",
    "parser.add_argument('--imagenet_default_mean_and_std', default=True, action='store_true')\n",
    "parser.add_argument('--num_segments', type=int, default= 1)\n",
    "parser.add_argument('--num_frames', type=int, default= 16)\n",
    "parser.add_argument('--sampling_rate', type=int, default= 4)\n",
    "parser.add_argument('--data_set', default='Kinetics-400', choices=['Kinetics-400', 'SSV2', 'UCF101', 'HMDB51','image_folder'],\n",
    "                    type=str, help='dataset')\n",
    "parser.add_argument('--output_dir', default='',\n",
    "                    help='path where to save, empty for no saving')\n",
    "parser.add_argument('--log_dir', default=None,\n",
    "                    help='path where to tensorboard log')\n",
    "# parser.add_argument('--device', default='cuda',\n",
    "#                     help='device to use for training / testing')\n",
    "parser.add_argument('--seed', default=0, type=int)\n",
    "parser.add_argument('--resume', default='',\n",
    "                    help='resume from checkpoint')\n",
    "parser.add_argument('--auto_resume', action='store_true')\n",
    "parser.add_argument('--no_auto_resume', action='store_false', dest='auto_resume')\n",
    "parser.set_defaults(auto_resume=True)\n",
    "\n",
    "parser.add_argument('--save_ckpt', action='store_true')\n",
    "parser.add_argument('--no_save_ckpt', action='store_false', dest='save_ckpt')\n",
    "parser.set_defaults(save_ckpt=True)\n",
    "\n",
    "parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                    help='start epoch')\n",
    "parser.add_argument('--eval', action='store_true',\n",
    "                    help='Perform evaluation only')\n",
    "parser.add_argument('--dist_eval', action='store_true', default=False,\n",
    "                    help='Enabling distributed evaluation')\n",
    "parser.add_argument('--num_workers', default=10, type=int)\n",
    "parser.add_argument('--pin_mem', action='store_true',\n",
    "                    help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "parser.set_defaults(pin_mem=True)\n",
    "\n",
    "# distributed training parameters\n",
    "parser.add_argument('--world_size', default=1, type=int,\n",
    "                    help='number of distributed processes')\n",
    "parser.add_argument('--local_rank', default=-1, type=int)\n",
    "parser.add_argument('--dist_on_itp', action='store_true')\n",
    "parser.add_argument('--dist_url', default='env://',\n",
    "                    help='url used to set up distributed training')\n",
    "\n",
    "parser.add_argument('--enable_deepspeed', action='store_true', default=False)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"cifar10\")\n",
    "parser.add_argument(\"--concept_set\", type=str, default=None, \n",
    "                    help=\"path to concept set name\")\n",
    "parser.add_argument(\"--backbone\", type=str, default=\"clip_RN50\", help=\"Which pretrained model to use as backbone\")\n",
    "parser.add_argument(\"--clip_name\", type=str, default=\"ViT-B/16\", help=\"Which CLIP model to use\")\n",
    "\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\", help=\"Which device to use\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=512, help=\"Batch size used when saving model/CLIP activations\")\n",
    "parser.add_argument(\"--saga_batch_size\", type=int, default=256, help=\"Batch size used when fitting final layer\")\n",
    "parser.add_argument(\"--proj_batch_size\", type=int, default=50000, help=\"Batch size to use when learning projection layer\")\n",
    "\n",
    "parser.add_argument(\"--feature_layer\", type=str, default='layer4', \n",
    "                    help=\"Which layer to collect activations from. Should be the name of second to last layer in the model\")\n",
    "parser.add_argument(\"--activation_dir\", type=str, default='saved_activations', help=\"save location for backbone and CLIP activations\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='saved_models', help=\"where to save trained models\")\n",
    "parser.add_argument(\"--clip_cutoff\", type=float, default=0.25, help=\"concepts with smaller top5 clip activation will be deleted\")\n",
    "parser.add_argument(\"--proj_steps\", type=int, default=1000, help=\"how many steps to train the projection layer for\")\n",
    "parser.add_argument(\"--interpretability_cutoff\", type=float, default=0.45, help=\"concepts with smaller similarity to target concept will be deleted\")\n",
    "parser.add_argument(\"--lam\", type=float, default=0.0007, help=\"Sparsity regularization parameter, higher->more sparse\")\n",
    "parser.add_argument(\"--n_iters\", type=int, default=1000, help=\"How many iterations to run the final layer solver for\")\n",
    "parser.add_argument(\"--print\", action='store_true', help=\"Print all concepts being deleted in this stage\")\n",
    "parser.add_argument('--data_path', default='data/video_annotation/ucf101', type=str,\n",
    "                    help='dataset path')\n",
    "parser.add_argument('--video-anno-path',type=str)\n",
    "parser.add_argument('--center_frame',action='store_true')\n",
    "parser.add_argument('--no_aug',type=bool,default=False)\n",
    "parser.add_argument('--dual_encoder', default='clip', choices=['clip', 'lavila', 'internvid'],\n",
    "                    type=str, help='dataset')\n",
    "parser.add_argument('--dual_encoder_frames',type=int,default=16)\n",
    "parser.add_argument('--lavila_ckpt',type=str,default=None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['notebook_name',\n",
    "            '--data_set', 'SSV2',\n",
    "            '--nb_classes', '174',\n",
    "            '--video-anno-path', '/data/jong980812/project/Video-CBM/data/video_annotation/ssv2',\n",
    "            '--data_path', '/local_datasets/something-something/something-something-v2-mp4',\n",
    "            '--lavila_ckpt','/data/datasets/video_checkpoint/lavila_TSF_B.pth',\n",
    "            \"--finetune\",\"/data/datasets/video_checkpoint/ssv2/ssv2_finetune.pth\",\n",
    "            \"--backbone\",\"vmae_vit_base_patch16_224\"\n",
    "            ]\n",
    "argument = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5720aa2-4365-4bd3-b2e8-c52217de46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import cbm_utils\n",
    "import data_utils\n",
    "import json\n",
    "\n",
    "import cbm\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "argument.nb_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486050c-7a36-4339-b9ff-e6b8426dbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the  correct model dir, everything else should be taken care of\n",
    "load_dir = \"/data/jong980812/project/Video-CBM_backup/results/ssv2/finetune_internvid_ost_spatio/model/SSV2_cbm_2024_08_01_21_45\"\n",
    "device = \"cuda\"\n",
    "\n",
    "with open(os.path.join(load_dir, \"args.txt\"), \"r\") as f:\n",
    "    args = json.load(f)\n",
    "dataset = args[\"data_set\"]\n",
    "_, target_preprocess = data_utils.get_target_model(args[\"backbone\"], device,argument)\n",
    "print('done')\n",
    "model = cbm.load_cbm(load_dir, device,argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c543cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e2bb6-d79d-4de3-bdf4-6b9c3b466682",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_d_probe = dataset+\"_val\"\n",
    "cls_file = data_utils.LABEL_FILES[dataset]\n",
    "\n",
    "val_data_t = data_utils.get_data(val_d_probe,args=argument)\n",
    "argument.no_aug = True\n",
    "argument.center_frame = True\n",
    "val_pil_data = data_utils.get_data(val_d_probe,args=argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_t.no_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871a0d2-8b18-4bac-aa26-bf3dce49fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cls_file, \"r\") as f:\n",
    "    classes = f.read().split(\"\\n\")\n",
    "\n",
    "with open(os.path.join(load_dir, \"concepts.txt\"), \"r\") as f:\n",
    "    concepts = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ecd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc3768",
   "metadata": {},
   "source": [
    "## Measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce81765",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cbm_utils.get_accuracy_cbm(model, val_data_t, device,64,8)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb41545",
   "metadata": {},
   "source": [
    "## Show final layer weights for some classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201ed8e-bbef-4b5a-ab55-4bed2bbb235e",
   "metadata": {},
   "source": [
    "You can build a Sankey diagram of weights by copying the incoming weights printed below into https://sankeymatic.com/build/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f95bd7-a06c-49d0-a7d4-3a190a6083da",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = random.choices([i for i in range(len(classes))], k=1)\n",
    "\n",
    "for i in to_show:\n",
    "    print(\"Output class:{} - {}\".format(i, classes[i]))\n",
    "    print(\"Incoming weights:\")\n",
    "    for j in range(len(concepts)):\n",
    "        if torch.abs(model.final.weight[i,j])>0.05:\n",
    "            print(\"{} [{:.4f}] {}\".format(concepts[j], model.final.weight[i,j], classes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2b1f0-03e2-41f9-95de-bbfff66f61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = random.choices([i for i in range(len(classes))], k=2)\n",
    "\n",
    "top_weights, top_weight_ids = torch.topk(model.final.weight, k=5, dim=1)\n",
    "bottom_weights, bottom_weight_ids = torch.topk(model.final.weight, k=5, dim=1, largest=False)\n",
    "\n",
    "for i in to_show:\n",
    "    print(\"Class {} - {}\".format(i, classes[i]))\n",
    "    out = \"Highest weights: \"\n",
    "    for j in range(top_weights.shape[1]):\n",
    "        idx = int(top_weight_ids[i, j].cpu())\n",
    "        out += \"{}:{:.3f}, \".format(concepts[idx], top_weights[i, j])\n",
    "    print(out)\n",
    "    out = \"Lowest weights: \"\n",
    "    for j in range(bottom_weights.shape[1]):\n",
    "        idx = int(bottom_weight_ids[i, j].cpu())\n",
    "        out += \"{}:{:.3f}, \".format(concepts[idx], bottom_weights[i, j])\n",
    "    print(out + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268a22b-5ca4-4fd5-a338-89bf98349f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some features may not have any non-zero outgoing weights, \n",
    "# i.e. these are not used by the model and should be deleted for better performance\n",
    "weight_contribs = torch.sum(torch.abs(model.final.weight), dim=0)\n",
    "print(\"Num concepts with outgoing weights:{}/{}\".format(torch.sum(weight_contribs>1e-5), len(weight_contribs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def avi_to_mp4(video_path):\n",
    "    mp4_path = video_path[:-3]+'mp4'\n",
    "    if os.path.exists(mp4_path):\n",
    "        print(f\"The file '{mp4_path}' exists.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"The file '{mp4_path}' does not exist.\")\n",
    "    input_file = video_path\n",
    "    output_file = mp4_path\n",
    "    command = ['ffmpeg', '-i', input_file, '-q:v', '2', output_file]\n",
    "\n",
    "    # FFmpeg 명령어 실행\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "    print(\"Video conversion completed successfully.\")\n",
    "    return mp4_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f58971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1024c31d",
   "metadata": {},
   "source": [
    "## Explain model reasoning for random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed063b1a-315d-4527-bae4-53848b27d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "from IPython.display import display, HTML\n",
    "to_display = random.sample([i for i in range(len(val_pil_data))], k=4)\n",
    "# to_display = [5,234,122,52,999]\n",
    "to_pil_image = ToPILImage()\n",
    "\n",
    "# val_pil_data.get_sample_path=True\n",
    "with torch.no_grad():\n",
    "    for i in to_display:\n",
    "        image, label = val_pil_data[i]\n",
    "        # video_path = avi_to_mp4(video_path)\n",
    "        # video_html = f'''\n",
    "        # <video width=\"640\" height=\"480\" controls>\n",
    "        #   <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "        #   Your browser does not support the video tag.\n",
    "        # </video>\n",
    "        # '''\n",
    "        x, _ = val_data_t[i]\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        # image = to_pil_image(image)\n",
    "        # display(HTML(video_html))\n",
    "        image = to_pil_image(image)\n",
    "        display(image.resize([540,540]))\n",
    "        outputs, concept_act = model(x)\n",
    "        \n",
    "        top_logit_vals, top_classes = torch.topk(outputs[0], dim=0, k=2)\n",
    "        conf = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "        print(\"Image:{} Gt:{}, 1st Pred:{}, {:.3f}, 2nd Pred:{}, {:.3f}\".format(i, classes[int(label)], classes[top_classes[0]], top_logit_vals[0],\n",
    "                                                                      classes[top_classes[1]], top_logit_vals[1]))\n",
    "        \n",
    "        for k in range(1):\n",
    "            contributions = concept_act[0]*model.final.weight[top_classes[k], :]\n",
    "            feature_names = [(\"NOT \" if concept_act[0][i] < 0 else \"\") + concepts[i] for i in range(len(concepts))]\n",
    "            values = contributions.cpu().numpy()\n",
    "            max_display = min(int(sum(abs(values)>0.005))+1, 8)\n",
    "            title = \"Pred:{} - Conf: {:.3f} - Logit:{:.2f} - Bias:{:.2f}\".format(classes[top_classes[k]],\n",
    "                             conf[top_classes[k]], top_logit_vals[k], model.final.bias[top_classes[k]])\n",
    "            plots.bar(values, feature_names, max_display=max_display, title=title, fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c74c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91061308",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d410c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(val_pil_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_d_probe = 'cifar10'+\"_val\"\n",
    "cls_file = data_utils.LABEL_FILES[dataset]\n",
    "\n",
    "val_data_t = data_utils.get_data(val_d_probe, preprocess=target_preprocess,args=argument)\n",
    "val_pil_data = data_utils.get_data(val_d_probe,args=argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150331d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from viclip import get_viclip, retrieve_text, _frame_from_video\n",
    "\n",
    "# modify xxx to the path of the pretrained model\n",
    "model_cfgs = {\n",
    "    'viclip-l-internvid-10m-flt': {\n",
    "        'size': 'l',\n",
    "        'pretrained': 'xxx/ViCLIP-L_InternVid-FLT-10M.pth',\n",
    "    },\n",
    "    'viclip-l-internvid-200m': {\n",
    "        'size': 'l',\n",
    "        'pretrained': 'xxx/ViCLIP-L_InternVid-200M.pth',\n",
    "    },\n",
    "    'viclip-b-internvid-10m-flt': {\n",
    "        'size': 'b',\n",
    "        'pretrained': 'xxx/ViCLIP-B_InternVid-FLT-10M.pth',\n",
    "    },\n",
    "    'viclip-b-internvid-200m': {\n",
    "        'size': 'b',\n",
    "        'pretrained': '/data/datasets/video_checkpoint/ViCLIP-B_InternVid-200M.pth',\n",
    "    },\n",
    "}\n",
    "ckpt='/data/datasets/video_checkpoint/ViCLIP-B_InternVid-200M.pth'\n",
    "cfg = model_cfgs['viclip-b-internvid-200m']\n",
    "model_l = get_viclip(cfg['size'], cfg['pretrained'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6121add",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l['viclip'].tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l['tokenizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df52903",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-cbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
